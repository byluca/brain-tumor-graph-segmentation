{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bccb632-8467-473e-838c-1834fef4092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazione delle librerie necessarie\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import platform\n",
    "# Impostazione dei percorsi\n",
    "_base_path = '\\\\'.join(os.getcwd().split('\\\\')[:-1]) + '\\\\' if platform =='win32' else '/'.join(os.getcwd().split('/')[:-1]) + '/'\n",
    "sys.path.append(_base_path)\n",
    "# Importare le librerie necessarie\n",
    "from monai.utils import set_determinism\n",
    "from src.helpers.config import get_config\n",
    "from src.models.gnn import GraphSAGE, GAT, ChebNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "from sklearn.linear_model import Ridge\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8104d280-1fba-442e-b69a-aff284a21b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione dei percorsi\n",
    "_config = get_config()\n",
    "data_path = os.path.join(_base_path, _config.get('DATA_FOLDER'))\n",
    "graph_path = os.path.join(data_path, _config.get('GRAPH_FOLDER'))\n",
    "saved_path = os.path.join(_base_path, _config.get('SAVED_FOLDER'))\n",
    "reports_path = os.path.join(_base_path, _config.get('REPORT_FOLDER'))\n",
    "logs_path = os.path.join(_base_path, _config.get('LOG_FOLDER'))\n",
    "cache_path = os.path.join(_base_path, 'cache')\n",
    "os.makedirs(cache_path, exist_ok=True)\n",
    "if platform == 'win32':\n",
    "    data_path = data_path.replace('/', '\\\\')\n",
    "    graph_path = graph_path.replace('/', '\\\\')\n",
    "    saved_path = saved_path.replace('/', '\\\\')\n",
    "    reports_path = reports_path.replace('/', '\\\\')\n",
    "    logs_path = logs_path.replace('/', '\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd69987-a68c-4262-8b87-cc1dfe9557ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ec232e3f4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impostare un seed per la riproducibilità\n",
    "set_determinism(seed=3)\n",
    "random.seed(3)\n",
    "np.random.seed(3)\n",
    "torch.manual_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c5dc16-98fc-412c-ae2f-03d74c3af934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione dei parametri del modello\n",
    "# PARAMETRI CONDIVISI\n",
    "num_node_features = 50 # Dimensione feature di input\n",
    "num_classes = 4 # Numero di classi di output (0=Non-tumore,1=NCR, 2=ED, 3=ET)\n",
    "lr = 1e-4 # Learning rate per l'ottimizzatore\n",
    "weight_decay = 1e-5 # Weight decay per l'ottimizzatore\n",
    "dropout = .0 # Probabilità di dropout (per features)\n",
    "hidden_channels = [512, 512, 512, 512, 512, 512, 512] # Unità nascoste\n",
    "# PARAMETRI SPECIFICI PER IL MODELLO CHEBNET\n",
    "k = 4 # Ordine polinomiale Chebyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed80bfee-f83e-4365-a167-ef9e6b6e01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello creato: ChebNet\n"
     ]
    }
   ],
   "source": [
    "# Creazione del modello da utilizzare (ChebNet)\n",
    "model = ChebNet(\n",
    "in_channels=num_node_features,\n",
    "hidden_channels=hidden_channels,\n",
    "out_channels=num_classes,\n",
    "dropout=dropout,\n",
    "K=k\n",
    ")\n",
    "print(f\"Modello creato: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c23e5e-2f28-45aa-8c4f-6ef325286281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per trovare e caricare un grafo per l'analisi\n",
    "def find_and_load_graph(subject_id=None):\n",
    "    \"\"\"\n",
    "    Trova e carica un grafo per l'analisi.\n",
    "    Args:\n",
    "    subject_id: ID specifico del soggetto da caricare, se None ne verrà scelto uno casualmente\n",
    "    Returns:\n",
    "    data: Il grafo caricato\n",
    "    subject_id: L'ID del soggetto caricato\n",
    "    \"\"\"\n",
    "    if subject_id is None:\n",
    "        # Trova le cartelle dei soggetti che contengono grafi\n",
    "        subject_dirs = [d for d in os.listdir(graph_path) if os.path.isdir(os.path.join(graph_path, d))]\n",
    "        valid_subjects = []\n",
    "# Cerca i primi 10 soggetti che hanno file .graph\n",
    "        for subject in subject_dirs[:100]: # Limita la ricerca per efficienza\n",
    "            graph_file = os.path.join(graph_path, subject, f\"{subject}.graph\")\n",
    "            if os.path.isfile(graph_file):\n",
    "                valid_subjects.append(subject)\n",
    "                if len(valid_subjects) >= 10:\n",
    "                    break\n",
    "        if not valid_subjects:\n",
    "            raise FileNotFoundError(\"Nessun grafo trovato nella directory data/graphs/\")\n",
    "# Scegli un soggetto casuale\n",
    "        subject_id = random.choice(valid_subjects)\n",
    "# Carica il grafo\n",
    "    graph_file = os.path.join(graph_path, subject_id, f\"{subject_id}.graph\")\n",
    "    if not os.path.isfile(graph_file):\n",
    "        raise FileNotFoundError(f\"File grafo non trovato per il soggetto{subject_id}\")\n",
    "    print(f\"Caricamento grafo: {graph_file}\")\n",
    "    data = torch.load(graph_file)\n",
    "    return data, subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aab88d3-fe23-4b19-9a86-e8f4941c5810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento grafo: /home/gianuca/Scrivania/Tesi/Progetto/brain-tumor-graph-segmentation-main/brain-tumor-graph-segmentation-main/data/graphs/BraTS-GLI-01166-000/BraTS-GLI-01166-000.graph\n",
      "Grafo caricato con successo: BraTS-GLI-01166-000\n",
      "Numero di nodi: 2607\n",
      "Numero di archi: 26070\n",
      "Numero di features per nodo: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298044/1901407787.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(graph_file)\n"
     ]
    }
   ],
   "source": [
    "# Carica un grafo specifico con alta accuratezza\n",
    "subject_id = \"BraTS-GLI-01166-000\" # Grafo con accuratezza 100% (lo stesso del notebook 4 per confronto)\n",
    "try:\n",
    "    data, subject_id = find_and_load_graph(subject_id)\n",
    "    print(f\"Grafo caricato con successo: {subject_id}\")\n",
    "    print(f\"Numero di nodi: {data.x.shape[0]}\")\n",
    "    print(f\"Numero di archi: {data.edge_index.shape[1]}\")\n",
    "    print(f\"Numero di features per nodo: {data.x.shape[1]}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Errore: {e}\")\n",
    "    print(\"Tentativo di caricamento di un grafo alternativo...\")\n",
    "    data, subject_id = find_and_load_graph(None)\n",
    "    print(f\"Grafo alternativo caricato: {subject_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aa42ee3-5afa-4b86-986c-976afdd07e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilizzo del modello pre-addestrato: CHEBNET_1739029370_best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_298044/4237471290.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(saved_path, latest_model),map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChebNet(\n",
       "  (layers): ModuleList(\n",
       "    (0): ChebConv(50, 512, K=4, normalization=sym)\n",
       "    (1-6): 6 x ChebConv(512, 512, K=4, normalization=sym)\n",
       "    (7): ChebConv(512, 4, K=4, normalization=sym)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carica il modello pre-addestrato\n",
    "model_files = [f for f in os.listdir(saved_path) if 'CHEBNET' in f and f.endswith('_best.pth')]\n",
    "if not model_files:\n",
    "    raise FileNotFoundError(\"Nessun modello ChebNet pre-addestrato trovato nella directory saved/\")\n",
    "latest_model = model_files[-1]\n",
    "print(f\"Utilizzo del modello pre-addestrato: {latest_model}\")\n",
    "model.load_state_dict(torch.load(os.path.join(saved_path, latest_model),map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c558203-85b7-43a3-bdf0-426b2a647eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza sul grafo BraTS-GLI-01166-000: 1.0000\n",
      "Distribuzione classi predette: [2550   16    4   37]\n"
     ]
    }
   ],
   "source": [
    "# Verifica dell'accuratezza sul grafo caricato\n",
    "with torch.no_grad():\n",
    "    outputs = model(data.x, data.edge_index.type(torch.int64))\n",
    "    predicted_labels = outputs.argmax(dim=1)\n",
    "    # Crea un tensore delle classi da usare, contenente le predizioni del modello\n",
    "    node_classes = predicted_labels.clone()\n",
    "    # Calcola l'accuratezza usando predicted_labels e data.y se possibile\n",
    "    try:\n",
    "        accuracy = (predicted_labels == data.y).float().mean().item()\n",
    "        print(f\"Accuratezza sul grafo {subject_id}: {accuracy:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Impossibile calcolare l'accuratezza usando data.y: {e}\")\n",
    "        print(\"Usando solo le predizioni per l'analisi\")\n",
    "    class_counts = torch.bincount(predicted_labels, minlength=num_classes)\n",
    "    print(f\"Distribuzione classi predette: {class_counts.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fc5f591-d4bb-4f2a-8104-76011a879e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_explainer_simple(node_idx, x, edge_index):\n",
    "    \"\"\"\n",
    "    Implementazione semplificata di GNNExplainer basata sui gradienti.\n",
    "    \"\"\"\n",
    "    # Assicurati che node_idx sia un tensore 1D e mantieni una versione scalare\n",
    "    if isinstance(node_idx, int):\n",
    "        node_idx_tensor = torch.tensor([node_idx], dtype=torch.int64)\n",
    "        node_idx_scalar = node_idx\n",
    "    elif isinstance(node_idx, torch.Tensor) and node_idx.dim() == 0:\n",
    "        node_idx_tensor = node_idx.unsqueeze(0)\n",
    "        node_idx_scalar = node_idx.item()\n",
    "    else:\n",
    "        node_idx_tensor = node_idx.to(torch.int64)\n",
    "        node_idx_scalar = node_idx.item() if node_idx.numel() == 1 else node_idx[0].item()\n",
    "    # Clona i dati di input e abilita il calcolo dei gradienti\n",
    "    x_grad = x.clone().detach().requires_grad_(True)\n",
    "    # Forward pass - AGGIUNGI .type(torch.int64) a edge_index\n",
    "    with torch.enable_grad():\n",
    "        outputs = model(x_grad, edge_index.type(torch.int64))\n",
    "        pred_class = outputs[node_idx_scalar].argmax().item()\n",
    "        # Calcola il gradiente rispetto alla classe predetta\n",
    "        model.zero_grad()\n",
    "        outputs[node_idx_scalar, pred_class].backward()\n",
    "        # Usa il gradiente delle feature del nodo come misura di importanza\n",
    "        node_importance = x_grad.grad[node_idx_scalar].abs()\n",
    "        return node_importance, pred_class\n",
    "def gradcam_explainer(node_idx, x, edge_index):\n",
    "    \"\"\"\n",
    "    Implementazione di GradCAM per GNN.\n",
    "    Args:\n",
    "    node_idx: Indice del nodo da spiegare\n",
    "    x: Feature dei nodi\n",
    "    edge_index: Indici degli archi\n",
    "    Returns:\n",
    "    node_importance: Importanza delle feature\n",
    "    pred_class: Classe predetta\n",
    "    \"\"\"\n",
    "    # Assicurati che node_idx sia un tensore 1D e mantieni una versione scalare\n",
    "    if isinstance(node_idx, int):\n",
    "        node_idx_tensor = torch.tensor([node_idx], dtype=torch.int64)\n",
    "        node_idx_scalar = node_idx\n",
    "    elif isinstance(node_idx, torch.Tensor) and node_idx.dim() == 0:\n",
    "        node_idx_tensor = node_idx.unsqueeze(0)\n",
    "        node_idx_scalar = node_idx.item()\n",
    "    else:\n",
    "        node_idx_tensor = node_idx.to(torch.int64)\n",
    "        node_idx_scalar = node_idx.item() if node_idx.numel() == 1 else node_idx[0].item()\n",
    "    # Simile a GNNExplainer ma con pesi diversi\n",
    "    x_grad = x.clone().detach().requires_grad_(True)\n",
    "    with torch.enable_grad():\n",
    "        outputs = model(x_grad, edge_index.type(torch.int64))\n",
    "        pred_class = outputs[node_idx_scalar].argmax().item()\n",
    "        # Backpropagation\n",
    "        model.zero_grad()\n",
    "        outputs[node_idx_scalar, pred_class].backward()\n",
    "        # GradCAM pondera i gradienti\n",
    "        gradients = x_grad.grad[node_idx_scalar]\n",
    "        node_importance = gradients * x[node_idx_scalar] # Moltiplica per l'attivazione\n",
    "        node_importance = node_importance.abs()\n",
    "    return node_importance, pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d51b8-c08b-4dca-9486-78b49526e131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
